{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqdQG6aYbQww",
    "outputId": "9a81ce3d-d7d6-4910-c12b-2f1108a05535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.20.3)\n",
      "Requirement already satisfied: nlp in /opt/conda/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from nlp) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from nlp) (12.0.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from nlp) (2.0.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from nlp) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.40.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->nlp) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->nlp) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: torchtext in /opt/conda/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext) (2.29.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext) (1.24.3)\n",
      "Requirement already satisfied: torchdata in /opt/conda/lib/python3.10/site-packages (from torchtext) (0.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers portalocker accelerate nlp scikit-learn matplotlib\n",
    "!pip install --upgrade torch torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ctBUDV2gbjM-"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from nlp import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "data = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x1uGYn0z46l4"
   },
   "outputs": [],
   "source": [
    "train_dset = data['train']\n",
    "test_dset = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y38nJlQ47hp2"
   },
   "outputs": [],
   "source": [
    "idxs = np.arange(len(train_dset))\n",
    "train_idxs, val_idxs = train_test_split(idxs, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dset = train_dset.select(train_idxs)\n",
    "val_dset = train_dset.select(val_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jxS_V2afTioo"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BrqtFbBDyauN"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertModel,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImStbiphAfhL",
    "outputId": "90d39026-5b14-4273-e70c-0e06f0f7256d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None)}, num_rows: 20000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZ7ldQUrBfqn",
    "outputId": "bf1cbc50-2e70-44e9-88fe-00550759031d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "id": "qiaAJ9hTQshp",
    "outputId": "1f16c927-360e-4107-d7b8-1b183f360cc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.310e+02, 2.025e+03, 5.718e+03, 3.418e+03, 2.110e+03, 1.418e+03,\n",
       "        1.070e+03, 7.590e+02, 5.840e+02, 4.630e+02, 3.440e+02, 2.760e+02,\n",
       "        2.030e+02, 1.530e+02, 1.380e+02, 1.110e+02, 1.010e+02, 6.500e+01,\n",
       "        4.900e+01, 9.500e+01, 4.800e+01, 6.000e+00, 0.000e+00, 2.000e+00,\n",
       "        1.000e+00, 1.000e+00, 2.000e+00, 2.000e+00, 1.000e+00, 1.000e+00,\n",
       "        1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00,\n",
       "        0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 1.000e+00]),\n",
       " array([  10. ,   59.2,  108.4,  157.6,  206.8,  256. ,  305.2,  354.4,\n",
       "         403.6,  452.8,  502. ,  551.2,  600.4,  649.6,  698.8,  748. ,\n",
       "         797.2,  846.4,  895.6,  944.8,  994. , 1043.2, 1092.4, 1141.6,\n",
       "        1190.8, 1240. , 1289.2, 1338.4, 1387.6, 1436.8, 1486. , 1535.2,\n",
       "        1584.4, 1633.6, 1682.8, 1732. , 1781.2, 1830.4, 1879.6, 1928.8,\n",
       "        1978. , 2027.2, 2076.4, 2125.6, 2174.8, 2224. , 2273.2, 2322.4,\n",
       "        2371.6, 2420.8, 2470. ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGiCAYAAAAWdZeEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqZUlEQVR4nO3de3CUVZ7G8ScBuglgJwRIdyIXwzADRG4CY+hVGZEsDWZmveCWIIMsIhZMcAeigJlxEHFqYHEVUbmM6yVurQxirVdQMAYBkeaWJcpFs6Jhg0InrphuUEiAnP3DzVu2XBtCkgPfT9VblX7P7z19ziF2Ht9+3+44Y4wRAACAZeIbegAAAADnghADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKwUc4j56quv9Nvf/lZt2rRRQkKCevbsqa1btzrtxhjNmDFDqampSkhIUFZWlj777LOoPg4cOKBRo0bJ4/EoKSlJ48aN06FDh6JqPv74Y1133XVq3ry5OnTooLlz557jFAEAwMUophDz7bff6pprrlGzZs30zjvvaNeuXXrsscfUunVrp2bu3Ll68skntXjxYm3atEktW7ZUIBDQkSNHnJpRo0Zp586dKigo0PLly7Vu3Trdc889TnskEtGQIUPUqVMnFRUV6dFHH9XMmTP1zDPP1MGUAQDAxSAuli+AfOCBB/Thhx/qgw8+OGm7MUZpaWm67777dP/990uSwuGwvF6v8vPzNWLECH3yySfKyMjQli1b1L9/f0nSypUrdeONN+rLL79UWlqaFi1apD/+8Y8KhUJyuVzOc7/++uv69NNPz3fOAADgItA0luI333xTgUBA//iP/6i1a9fq8ssv1+9+9zuNHz9eklRaWqpQKKSsrCznmMTERGVmZioYDGrEiBEKBoNKSkpyAowkZWVlKT4+Xps2bdItt9yiYDCogQMHOgFGkgKBgP7lX/5F3377bdSZn1pVVVWqqqpyHtfU1OjAgQNq06aN4uLiYpkmAABoIMYYHTx4UGlpaYqPP/0bRjGFmC+++EKLFi1Sbm6u/vCHP2jLli3653/+Z7lcLo0ZM0ahUEiS5PV6o47zer1OWygUUkpKSvQgmjZVcnJyVE16evoJfdS2nSzEzJ49Ww8//HAs0wEAAI3U3r171b59+9PWxBRiampq1L9/f/3lL3+RJF111VXasWOHFi9erDFjxpz7SOtAXl6ecnNzncfhcFgdO3bU3r175fF4GnBkAADgbEUiEXXo0EGXXXbZGWtjCjGpqanKyMiI2te9e3f953/+pyTJ5/NJksrLy5WamurUlJeXq0+fPk5NRUVFVB/Hjh3TgQMHnON9Pp/Ky8ujamof19b8lNvtltvtPmG/x+MhxAAAYJmzuRQkpruTrrnmGpWUlETt++///m916tRJkpSeni6fz6fCwkKnPRKJaNOmTfL7/ZIkv9+vyspKFRUVOTWrV69WTU2NMjMznZp169bp6NGjTk1BQYG6du160reSAADApSemEDNlyhRt3LhRf/nLX7R7924tWbJEzzzzjHJyciT9kJomT56sP//5z3rzzTe1fft23XnnnUpLS9PNN98s6YczN0OHDtX48eO1efNmffjhh5o0aZJGjBihtLQ0SdIdd9whl8ulcePGaefOnXr55Zc1f/78qLeLAADAJc7E6K233jI9evQwbrfbdOvWzTzzzDNR7TU1NeZPf/qT8Xq9xu12m8GDB5uSkpKomm+++caMHDnStGrVyng8HjN27Fhz8ODBqJqPPvrIXHvttcbtdpvLL7/czJkzJ6ZxhsNhI8mEw+FYpwgAABpILH+/Y/qcGJtEIhElJiYqHA5zTQwAAJaI5e83350EAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFZq2tADuJRd8cCKM9bsmZNdDyMBAMA+nIkBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArBRTiJk5c6bi4uKitm7dujntR44cUU5Ojtq0aaNWrVpp+PDhKi8vj+qjrKxM2dnZatGihVJSUjR16lQdO3YsqmbNmjXq27ev3G63unTpovz8/HOfIQAAuCjFfCbmyiuv1P79+51t/fr1TtuUKVP01ltv6ZVXXtHatWu1b98+3XrrrU778ePHlZ2drerqam3YsEEvvvii8vPzNWPGDKemtLRU2dnZGjRokIqLizV58mTdfffdWrVq1XlOFQAAXEyaxnxA06by+Xwn7A+Hw3ruuee0ZMkS3XDDDZKkF154Qd27d9fGjRs1YMAAvfvuu9q1a5fee+89eb1e9enTR4888oimT5+umTNnyuVyafHixUpPT9djjz0mSerevbvWr1+vefPmKRAInHJcVVVVqqqqch5HIpFYpwYAACwS85mYzz77TGlpaercubNGjRqlsrIySVJRUZGOHj2qrKwsp7Zbt27q2LGjgsGgJCkYDKpnz57yer1OTSAQUCQS0c6dO52aH/dRW1Pbx6nMnj1biYmJztahQ4dYpwYAACwSU4jJzMxUfn6+Vq5cqUWLFqm0tFTXXXedDh48qFAoJJfLpaSkpKhjvF6vQqGQJCkUCkUFmNr22rbT1UQiER0+fPiUY8vLy1M4HHa2vXv3xjI1AABgmZjeTho2bJjzc69evZSZmalOnTpp2bJlSkhIqPPBxcLtdsvtdjfoGAAAQP05r1usk5KS9Itf/EK7d++Wz+dTdXW1Kisro2rKy8uda2h8Pt8JdyvVPj5TjcfjafCgBAAAGo/zCjGHDh3S559/rtTUVPXr10/NmjVTYWGh015SUqKysjL5/X5Jkt/v1/bt21VRUeHUFBQUyOPxKCMjw6n5cR+1NbV9AAAASDGGmPvvv19r167Vnj17tGHDBt1yyy1q0qSJRo4cqcTERI0bN065ubl6//33VVRUpLFjx8rv92vAgAGSpCFDhigjI0OjR4/WRx99pFWrVunBBx9UTk6O81bQhAkT9MUXX2jatGn69NNPtXDhQi1btkxTpkyp+9kDAABrxXRNzJdffqmRI0fqm2++Ubt27XTttddq48aNateunSRp3rx5io+P1/Dhw1VVVaVAIKCFCxc6xzdp0kTLly/XxIkT5ff71bJlS40ZM0azZs1yatLT07VixQpNmTJF8+fPV/v27fXss8+e9vZqAABw6YkzxpiGHsSFEIlElJiYqHA4LI/H09DDOakrHlhxxpo9c7LrYSQAADQOsfz95ruTAACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlQgxAADASucVYubMmaO4uDhNnjzZ2XfkyBHl5OSoTZs2atWqlYYPH67y8vKo48rKypSdna0WLVooJSVFU6dO1bFjx6Jq1qxZo759+8rtdqtLly7Kz88/n6ECAICLzDmHmC1btuivf/2revXqFbV/ypQpeuutt/TKK69o7dq12rdvn2699Van/fjx48rOzlZ1dbU2bNigF198Ufn5+ZoxY4ZTU1paquzsbA0aNEjFxcWaPHmy7r77bq1atepchwsAAC4y5xRiDh06pFGjRunf/u3f1Lp1a2d/OBzWc889p8cff1w33HCD+vXrpxdeeEEbNmzQxo0bJUnvvvuudu3apf/4j/9Qnz59NGzYMD3yyCNasGCBqqurJUmLFy9Wenq6HnvsMXXv3l2TJk3Sbbfdpnnz5tXBlAEAwMXgnEJMTk6OsrOzlZWVFbW/qKhIR48ejdrfrVs3dezYUcFgUJIUDAbVs2dPeb1epyYQCCgSiWjnzp1OzU/7DgQCTh8nU1VVpUgkErUBAICLV9NYD1i6dKn+67/+S1u2bDmhLRQKyeVyKSkpKWq/1+tVKBRyan4cYGrba9tOVxOJRHT48GElJCSc8NyzZ8/Www8/HOt0AACApWI6E7N37179/ve/10svvaTmzZtfqDGdk7y8PIXDYWfbu3dvQw8JAABcQDGFmKKiIlVUVKhv375q2rSpmjZtqrVr1+rJJ59U06ZN5fV6VV1drcrKyqjjysvL5fP5JEk+n++Eu5VqH5+pxuPxnPQsjCS53W55PJ6oDQAAXLxiCjGDBw/W9u3bVVxc7Gz9+/fXqFGjnJ+bNWumwsJC55iSkhKVlZXJ7/dLkvx+v7Zv366KigqnpqCgQB6PRxkZGU7Nj/uorantAwAAIKZrYi677DL16NEjal/Lli3Vpk0bZ/+4ceOUm5ur5ORkeTwe3XvvvfL7/RowYIAkaciQIcrIyNDo0aM1d+5chUIhPfjgg8rJyZHb7ZYkTZgwQU8//bSmTZumu+66S6tXr9ayZcu0YsWKupgzAAC4CMR8Ye+ZzJs3T/Hx8Ro+fLiqqqoUCAS0cOFCp71JkyZavny5Jk6cKL/fr5YtW2rMmDGaNWuWU5Oenq4VK1ZoypQpmj9/vtq3b69nn31WgUCgrocLAAAsFWeMMQ09iAshEokoMTFR4XC40V4fc8UDZz6ztGdOdj2MBACAxiGWv998dxIAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACs1begB4MyueGDFadv3zMmup5EAANB4cCYGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlWIKMYsWLVKvXr3k8Xjk8Xjk9/v1zjvvOO1HjhxRTk6O2rRpo1atWmn48OEqLy+P6qOsrEzZ2dlq0aKFUlJSNHXqVB07diyqZs2aNerbt6/cbre6dOmi/Pz8c58hAAC4KMUUYtq3b685c+aoqKhIW7du1Q033KCbbrpJO3fulCRNmTJFb731ll555RWtXbtW+/bt06233uocf/z4cWVnZ6u6ulobNmzQiy++qPz8fM2YMcOpKS0tVXZ2tgYNGqTi4mJNnjxZd999t1atWlVHUwYAABeDOGOMOZ8OkpOT9eijj+q2225Tu3bttGTJEt12222SpE8//VTdu3dXMBjUgAED9M477+jXv/619u3bJ6/XK0lavHixpk+frq+//loul0vTp0/XihUrtGPHDuc5RowYocrKSq1cufKsxxWJRJSYmKhwOCyPx3M+U7xgrnhgxRlr9szJPmPdnjnZdTUkAAAaVCx/v8/5mpjjx49r6dKl+u677+T3+1VUVKSjR48qKyvLqenWrZs6duyoYDAoSQoGg+rZs6cTYCQpEAgoEok4Z3OCwWBUH7U1tX2cSlVVlSKRSNQGAAAuXjGHmO3bt6tVq1Zyu92aMGGCXnvtNWVkZCgUCsnlcikpKSmq3uv1KhQKSZJCoVBUgKltr207XU0kEtHhw4dPOa7Zs2crMTHR2Tp06BDr1AAAgEViDjFdu3ZVcXGxNm3apIkTJ2rMmDHatWvXhRhbTPLy8hQOh51t7969DT0kAABwATWN9QCXy6UuXbpIkvr166ctW7Zo/vz5uv3221VdXa3KysqoszHl5eXy+XySJJ/Pp82bN0f1V3v30o9rfnpHU3l5uTwejxISEk45LrfbLbfbHet0AACApc77c2JqampUVVWlfv36qVmzZiosLHTaSkpKVFZWJr/fL0ny+/3avn27KioqnJqCggJ5PB5lZGQ4NT/uo7amtg8AAAApxjMxeXl5GjZsmDp27KiDBw9qyZIlWrNmjVatWqXExESNGzdOubm5Sk5Olsfj0b333iu/368BAwZIkoYMGaKMjAyNHj1ac+fOVSgU0oMPPqicnBznLMqECRP09NNPa9q0abrrrru0evVqLVu2TCtWnPlOHgAAcOmIKcRUVFTozjvv1P79+5WYmKhevXpp1apV+vu//3tJ0rx58xQfH6/hw4erqqpKgUBACxcudI5v0qSJli9frokTJ8rv96tly5YaM2aMZs2a5dSkp6drxYoVmjJliubPn6/27dvr2WefVSAQqKMpAwCAi8F5f05MY8XnxAAAYJ96+ZwYAACAhkSIAQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsFLThh4A6sYVD6w4bfueOdn1NBIAAOoHZ2IAAICVCDEAAMBKMYWY2bNn65e//KUuu+wypaSk6Oabb1ZJSUlUzZEjR5STk6M2bdqoVatWGj58uMrLy6NqysrKlJ2drRYtWiglJUVTp07VsWPHomrWrFmjvn37yu12q0uXLsrPzz+3GQIAgItSTCFm7dq1ysnJ0caNG1VQUKCjR49qyJAh+u6775yaKVOm6K233tIrr7yitWvXat++fbr11lud9uPHjys7O1vV1dXasGGDXnzxReXn52vGjBlOTWlpqbKzszVo0CAVFxdr8uTJuvvuu7Vq1ao6mDIAALgYxHRh78qVK6Me5+fnKyUlRUVFRRo4cKDC4bCee+45LVmyRDfccIMk6YUXXlD37t21ceNGDRgwQO+++6527dql9957T16vV3369NEjjzyi6dOna+bMmXK5XFq8eLHS09P12GOPSZK6d++u9evXa968eQoEAnU0dQAAYLPzujspHA5LkpKTkyVJRUVFOnr0qLKyspyabt26qWPHjgoGgxowYICCwaB69uwpr9fr1AQCAU2cOFE7d+7UVVddpWAwGNVHbc3kyZNPOZaqqipVVVU5jyORyPlM7byd6W4hAABwfs75wt6amhpNnjxZ11xzjXr06CFJCoVCcrlcSkpKiqr1er0KhUJOzY8DTG17bdvpaiKRiA4fPnzS8cyePVuJiYnO1qFDh3OdGgAAsMA5h5icnBzt2LFDS5curcvxnLO8vDyFw2Fn27t3b0MPCQAAXEDn9HbSpEmTtHz5cq1bt07t27d39vt8PlVXV6uysjLqbEx5ebl8Pp9Ts3nz5qj+au9e+nHNT+9oKi8vl8fjUUJCwknH5Ha75Xa7z2U6AADAQjGdiTHGaNKkSXrttde0evVqpaenR7X369dPzZo1U2FhobOvpKREZWVl8vv9kiS/36/t27eroqLCqSkoKJDH41FGRoZT8+M+amtq+wAAAIjpTExOTo6WLFmiN954Q5dddplzDUtiYqISEhKUmJiocePGKTc3V8nJyfJ4PLr33nvl9/s1YMAASdKQIUOUkZGh0aNHa+7cuQqFQnrwwQeVk5PjnEmZMGGCnn76aU2bNk133XWXVq9erWXLlmnFCi6WBQAAP4jpTMyiRYsUDod1/fXXKzU11dlefvllp2bevHn69a9/reHDh2vgwIHy+Xx69dVXnfYmTZpo+fLlatKkifx+v37729/qzjvv1KxZs5ya9PR0rVixQgUFBerdu7cee+wxPfvss9xeDQAAHDGdiTHGnLGmefPmWrBggRYsWHDKmk6dOuntt98+bT/XX3+9tm3bFsvwAADAJYTvTgIAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYKWmDT0A1J8rHlhx2vY9c7LraSQAAJw/zsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgpaYNPQA0Llc8sOK07XvmZNfTSAAAOL2Yz8SsW7dOv/nNb5SWlqa4uDi9/vrrUe3GGM2YMUOpqalKSEhQVlaWPvvss6iaAwcOaNSoUfJ4PEpKStK4ceN06NChqJqPP/5Y1113nZo3b64OHTpo7ty5sc8OAABctGIOMd9995169+6tBQsWnLR97ty5evLJJ7V48WJt2rRJLVu2VCAQ0JEjR5yaUaNGaefOnSooKNDy5cu1bt063XPPPU57JBLRkCFD1KlTJxUVFenRRx/VzJkz9cwzz5zDFAEAwMUo5reThg0bpmHDhp20zRijJ554Qg8++KBuuukmSdK///u/y+v16vXXX9eIESP0ySefaOXKldqyZYv69+8vSXrqqad044036l//9V+Vlpaml156SdXV1Xr++eflcrl05ZVXqri4WI8//nhU2AEAAJeuOr2wt7S0VKFQSFlZWc6+xMREZWZmKhgMSpKCwaCSkpKcACNJWVlZio+P16ZNm5yagQMHyuVyOTWBQEAlJSX69ttvT/rcVVVVikQiURsAALh41WmICYVCkiSv1xu13+v1Om2hUEgpKSlR7U2bNlVycnJUzcn6+PFz/NTs2bOVmJjobB06dDj/CQEAgEbrornFOi8vT+Fw2Nn27t3b0EMCAAAXUJ2GGJ/PJ0kqLy+P2l9eXu60+Xw+VVRURLUfO3ZMBw4ciKo5WR8/fo6fcrvd8ng8URsAALh41WmISU9Pl8/nU2FhobMvEolo06ZN8vv9kiS/36/KykoVFRU5NatXr1ZNTY0yMzOdmnXr1uno0aNOTUFBgbp27arWrVvX5ZABAIClYg4xhw4dUnFxsYqLiyX9cDFvcXGxysrKFBcXp8mTJ+vPf/6z3nzzTW3fvl133nmn0tLSdPPNN0uSunfvrqFDh2r8+PHavHmzPvzwQ02aNEkjRoxQWlqaJOmOO+6Qy+XSuHHjtHPnTr388suaP3++cnNz62ziAADAbjHfYr1161YNGjTIeVwbLMaMGaP8/HxNmzZN3333ne655x5VVlbq2muv1cqVK9W8eXPnmJdeekmTJk3S4MGDFR8fr+HDh+vJJ5902hMTE/Xuu+8qJydH/fr1U9u2bTVjxgxurwYAAI6YQ8z1118vY8wp2+Pi4jRr1izNmjXrlDXJyclasmTJaZ+nV69e+uCDD2IdHgAAuERcNHcnAQCASwshBgAAWIkQAwAArBTzNTGQrnhgRUMPAQCASx5nYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIX9iJmZ7qwec+c7HoaCQDgUsaZGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJT6xFxcEn+oLALjQOBMDAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASnxODBsNnyQAAzgdnYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArMQt1mjUuA0bAHAqnIkBAABWIsQAAAArEWIAAICVuCYG1uO6GQC4NHEmBgAAWIkQAwAArESIAQAAViLEAAAAK3FhLy4JXPwLABcfzsQAAAArEWIAAICVeDsJ+H+85QQAdiHEADEg6ABA40GIAeoYQQcA6gchBmgA9Rl0CFUALlZc2AsAAKzUqM/ELFiwQI8++qhCoZB69+6tp556SldffXVDDwuoF5xBAYDTa7Qh5uWXX1Zubq4WL16szMxMPfHEEwoEAiopKVFKSkpDDw9oFOoq6BCYANio0b6d9Pjjj2v8+PEaO3asMjIytHjxYrVo0ULPP/98Qw8NAAA0Ao3yTEx1dbWKioqUl5fn7IuPj1dWVpaCweBJj6mqqlJVVZXzOBwOS5IikUidj6+m6vs67/NUIpHIGZ+PGmrqowYA6kPt640x5szFphH66quvjCSzYcOGqP1Tp041V1999UmPeeihh4wkNjY2NjY2totg27t37xnzQqM8E3Mu8vLylJub6zyuqanRgQMH1KZNG8XFxZ13/5FIRB06dNDevXvl8XjOuz+cHutdv1jv+sV61x/Wun7VxXobY3Tw4EGlpaWdsbZRhpi2bduqSZMmKi8vj9pfXl4un8930mPcbrfcbnfUvqSkpDofm8fj4T+EesR61y/Wu36x3vWHta5f57veiYmJZ1XXKC/sdblc6tevnwoLC519NTU1KiwslN/vb8CRAQCAxqJRnomRpNzcXI0ZM0b9+/fX1VdfrSeeeELfffedxo4d29BDAwAAjUCjDTG33367vv76a82YMUOhUEh9+vTRypUr5fV6G2Q8brdbDz300AlvWeHCYL3rF+tdv1jv+sNa16/6Xu84Y87mHiYAAIDGpVFeEwMAAHAmhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiDlLCxYs0BVXXKHmzZsrMzNTmzdvbughWWfmzJmKi4uL2rp16+a0HzlyRDk5OWrTpo1atWql4cOHn/CpzWVlZcrOzlaLFi2UkpKiqVOn6tixY/U9lUZp3bp1+s1vfqO0tDTFxcXp9ddfj2o3xmjGjBlKTU1VQkKCsrKy9Nlnn0XVHDhwQKNGjZLH41FSUpLGjRunQ4cORdV8/PHHuu6669S8eXN16NBBc+fOvdBTa5TOtN7/9E//dMLv+9ChQ6NqWO+zM3v2bP3yl7/UZZddppSUFN18880qKSmJqqmr1481a9aob9++crvd6tKli/Lz8y/09Bqds1nv66+//oTf7wkTJkTV1Mt6n/e3NV4Cli5dalwul3n++efNzp07zfjx401SUpIpLy9v6KFZ5aGHHjJXXnml2b9/v7N9/fXXTvuECRNMhw4dTGFhodm6dasZMGCA+bu/+zun/dixY6ZHjx4mKyvLbNu2zbz99tumbdu2Ji8vryGm0+i8/fbb5o9//KN59dVXjSTz2muvRbXPmTPHJCYmmtdff9189NFH5h/+4R9Menq6OXz4sFMzdOhQ07t3b7Nx40bzwQcfmC5dupiRI0c67eFw2Hi9XjNq1CizY8cO87e//c0kJCSYv/71r/U1zUbjTOs9ZswYM3To0Kjf9wMHDkTVsN5nJxAImBdeeMHs2LHDFBcXmxtvvNF07NjRHDp0yKmpi9ePL774wrRo0cLk5uaaXbt2maeeeso0adLErFy5sl7n29DOZr1/9atfmfHjx0f9fofDYae9vtabEHMWrr76apOTk+M8Pn78uElLSzOzZ89uwFHZ56GHHjK9e/c+aVtlZaVp1qyZeeWVV5x9n3zyiZFkgsGgMeaHPxrx8fEmFAo5NYsWLTIej8dUVVVd0LHb5qd/VGtqaozP5zOPPvqos6+ystK43W7zt7/9zRhjzK5du4wks2XLFqfmnXfeMXFxcearr74yxhizcOFC07p166j1nj59uunatesFnlHjdqoQc9NNN53yGNb73FVUVBhJZu3atcaYunv9mDZtmrnyyiujnuv22283gUDgQk+pUfvpehvzQ4j5/e9/f8pj6mu9eTvpDKqrq1VUVKSsrCxnX3x8vLKyshQMBhtwZHb67LPPlJaWps6dO2vUqFEqKyuTJBUVFeno0aNR69ytWzd17NjRWedgMKiePXtGfWpzIBBQJBLRzp0763ciliktLVUoFIpa38TERGVmZkatb1JSkvr37+/UZGVlKT4+Xps2bXJqBg4cKJfL5dQEAgGVlJTo22+/rafZ2GPNmjVKSUlR165dNXHiRH3zzTdOG+t97sLhsCQpOTlZUt29fgSDwag+amsu9df6n653rZdeeklt27ZVjx49lJeXp++//95pq6/1brRfO9BY/O///q+OHz9+wtcdeL1effrppw00KjtlZmYqPz9fXbt21f79+/Xwww/ruuuu044dOxQKheRyuU745nGv16tQKCRJCoVCJ/13qG3DqdWuz8nW78frm5KSEtXetGlTJScnR9Wkp6ef0EdtW+vWrS/I+G00dOhQ3XrrrUpPT9fnn3+uP/zhDxo2bJiCwaCaNGnCep+jmpoaTZ48Wddcc4169OghSXX2+nGqmkgkosOHDyshIeFCTKlRO9l6S9Idd9yhTp06KS0tTR9//LGmT5+ukpISvfrqq5Lqb70JMag3w4YNc37u1auXMjMz1alTJy1btuySfHHAxW3EiBHOzz179lSvXr30s5/9TGvWrNHgwYMbcGR2y8nJ0Y4dO7R+/fqGHsol4VTrfc899zg/9+zZU6mpqRo8eLA+//xz/exnP6u38fF20hm0bdtWTZo0OeEq9/Lycvl8vgYa1cUhKSlJv/jFL7R79275fD5VV1ersrIyqubH6+zz+U7671DbhlOrXZ/T/R77fD5VVFREtR87dkwHDhzg36AOdO7cWW3bttXu3bslsd7nYtKkSVq+fLnef/99tW/f3tlfV68fp6rxeDyX5P9onWq9TyYzM1OSon6/62O9CTFn4HK51K9fPxUWFjr7ampqVFhYKL/f34Ajs9+hQ4f0+eefKzU1Vf369VOzZs2i1rmkpERlZWXOOvv9fm3fvj3qhb+goEAej0cZGRn1Pn6bpKeny+fzRa1vJBLRpk2bota3srJSRUVFTs3q1atVU1PjvED5/X6tW7dOR48edWoKCgrUtWvXS/KtjVh8+eWX+uabb5SamiqJ9Y6FMUaTJk3Sa6+9ptWrV5/wFltdvX74/f6oPmprLrXX+jOt98kUFxdLUtTvd72s91lfAnwJW7p0qXG73SY/P9/s2rXL3HPPPSYpKSnqqmuc2X333WfWrFljSktLzYcffmiysrJM27ZtTUVFhTHmh1skO3bsaFavXm22bt1q/H6/8fv9zvG1t+wNGTLEFBcXm5UrV5p27dpxi/X/O3jwoNm2bZvZtm2bkWQef/xxs23bNvM///M/xpgfbrFOSkoyb7zxhvn444/NTTfddNJbrK+66iqzadMms379evPzn/886pbfyspK4/V6zejRo82OHTvM0qVLTYsWLS65W36NOf16Hzx40Nx///0mGAya0tJS895775m+ffuan//85+bIkSNOH6z32Zk4caJJTEw0a9asibql9/vvv3dq6uL1o/aW36lTp5pPPvnELFiw4JK8xfpM6717924za9Yss3XrVlNaWmreeOMN07lzZzNw4ECnj/pab0LMWXrqqadMx44djcvlMldffbXZuHFjQw/JOrfffrtJTU01LpfLXH755eb22283u3fvdtoPHz5sfve735nWrVubFi1amFtuucXs378/qo89e/aYYcOGmYSEBNO2bVtz3333maNHj9b3VBql999/30g6YRszZowx5ofbrP/0pz8Zr9dr3G63GTx4sCkpKYnq45tvvjEjR440rVq1Mh6Px4wdO9YcPHgwquajjz4y1157rXG73ebyyy83c+bMqa8pNiqnW+/vv//eDBkyxLRr1840a9bMdOrUyYwfP/6E//Fhvc/OydZZknnhhRecmrp6/Xj//fdNnz59jMvlMp07d456jkvFmda7rKzMDBw40CQnJxu32226dOlipk6dGvU5McbUz3rH/f+AAQAArMI1MQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJUIMAACw0v8BmPeBimILbs4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check sentence length distribution to understand if it makes sese to trim themt\n",
    "lengths = []\n",
    "for review in train_dset['text']:\n",
    "    lengths.append(len(review.split(' ')))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lengths, rwidth = 0.9, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BQkrJMoMCLem"
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_dset['text'], padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "val_encodings = tokenizer(val_dset['text'], padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "test_encodings = tokenizer(test_dset['text'], padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "\n",
    "y_train = train_dset['label']\n",
    "y_val = val_dset['label']\n",
    "y_test = test_dset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sb8In7lHDf5w"
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].to(device) for key, val in self.encodings.items()}\n",
    "        # import pdb; pdb.set_trace()\n",
    "        item['labels'] = torch.tensor(self.labels[idx]).to(device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aJdoT3QCy9nc"
   },
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     sentences = [item['text'] for item in batch]\n",
    "#     tokenized = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "#     input_ids = tokenized['input_ids']\n",
    "#     attention_mask = tokenized['attention_mask']\n",
    "#     labels = torch.tensor([item['label'] for item in batch], dtype=torch.float16k).to(device)\n",
    "\n",
    "#     return {\n",
    "#         'input_ids': input_ids.to(device),\n",
    "#         'attention_mask': attention_mask.to(device),\n",
    "#         'labels': labels.to(device)\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "23lnSbdf-KsM"
   },
   "outputs": [],
   "source": [
    "train_ds = IMDBDataset(train_encodings, y_train)\n",
    "val_ds = IMDBDataset(val_encodings, y_val)\n",
    "test_ds = IMDBDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "MhlKPFJH0QQx"
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "v4dCzned11et"
   },
   "outputs": [],
   "source": [
    "## from accelerate import accelerator\n",
    "from transformers import AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps = 500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./\",\n",
    "    logging_steps=10,\n",
    "    dataloader_pin_memory=True\n",
    ")\n",
    "\n",
    "class DistilBERTForBinaryClassification(nn.Module):\n",
    "    def __init__(self, num_classes, freeze_weights=True):\n",
    "        super(DistilBERTForBinaryClassification, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        if freeze_weights:\n",
    "            for param in self.distilbert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        # TODO: properly check dimensions of the layer\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            self.distilbert.config.hidden_size,\n",
    "            self.distilbert.config.hidden_size // 2,\n",
    "            kernel_size=7,\n",
    "            padding='same'\n",
    "        )\n",
    "\n",
    "        self.maxp = nn.MaxPool1d(128)\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.linear2 = nn.Linear(self.distilbert.config.hidden_size // 2, num_classes)\n",
    "\n",
    "        # self.dropout2 = nn.Dropout(0.1)\n",
    "        # self.linear2 = nn.Linear(self.distilbert.config.hidden_size // 2, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # pooled_output = self.distilbert(input_ids, attention_mask)\n",
    "        pooled_output = self.distilbert(input_ids, attention_mask)\n",
    "        # cls_token_state = pooled_output[0][:, 0, :]\n",
    "        pooled_output = pooled_output['last_hidden_state']\n",
    "        output = self.dropout1(pooled_output)\n",
    "        output = self.conv1d(output.permute(0,2,1))\n",
    "        output = self.maxp(output)\n",
    "        output = self.dropout2(output)\n",
    "        # TODO: figure if it is needed to permute everything back again\n",
    "        import pdb; pdb.set_trace()\n",
    "        output = self.linear2(output)\n",
    "        output = nn.Sigmoid()(output)  # Binary classification\n",
    "        # output = self.linear1(output)\n",
    "        # output = self.dropout2(output)\n",
    "        # output = self.linear2(output)\n",
    "        # output = nn.Sigmoid()(output)  # Binary classification\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "8oC94AaAcY95",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_dataloader, optimizer, device, prog_bar):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # for batch_idx, batch in enumerate(train_dataloader):\n",
    "    for batch_idx, batch in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        labels = labels.reshape(-1, 1).float()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = nn.BCELoss()(outputs, labels)\n",
    "        # loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "        # Get predictions\n",
    "        pred_labels = torch.round(outputs)\n",
    "\n",
    "        total_correct += torch.sum(pred_labels == labels.reshape(-1, 1)).item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        prog_bar.set_postfix({\n",
    "            'batch_accuracy': (total_correct / total_samples),\n",
    "            'batch_loss': loss.item()\n",
    "        })\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    return average_loss, accuracy\n",
    "\n",
    "def evaluate(model, eval_dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            labels = labels.reshape(-1, 1).float()\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = nn.BCELoss()(outputs, labels)\n",
    "            # loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "            # Get predictions and compute accuracy and cumulative loss\n",
    "            pred_labels = torch.round(outputs)\n",
    "            total_correct += torch.sum(pred_labels == labels.reshape(-1, 1)).item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            all_predictions.extend(pred_labels.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(eval_dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "    return average_loss, accuracy\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    progressively_unfreeze=False\n",
    "):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        progress_bar = tqdm(train_dataloader, desc=f'epoch {epoch +1}', unit='batch')\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train(model, train_dataloader, optimizer, device, progress_bar)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "        eval_loss, eval_acc = evaluate(model, eval_dataloader, device)\n",
    "        print(f\"Validation Loss: {eval_loss:.4f}, Validation Accuracy: {eval_acc:.4f}\")\n",
    "\n",
    "        # This is a naive attempt at progressively unfreezing distilbert parameters while training.x\n",
    "        # This is clearly not working for some reason\n",
    "        if progressively_unfreeze:\n",
    "            for param in list(model.distilbert.parameters())[-epoch:]:\n",
    "                 param.requires_grad = True\n",
    "\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjcyWVqOJxEM",
    "outputId": "f6a60abb-794e-4b42-c633-15c8ce3b4ae8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBERTForBinaryClassification(num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "DdoegowJdcdb",
    "outputId": "15ac3924-c85b-48fc-aca9-c1e729ff1f66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "epoch 1:   0%|          | 0/625 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "> \u001b[0;32m/tmp/ipykernel_32356/3495549829.py\u001b[0m(55)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     53 \u001b[0;31m        \u001b[0;31m# TODO: figure if it is needed to permute everything back again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 55 \u001b[0;31m        \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     56 \u001b[0;31m        \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Binary classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     57 \u001b[0;31m        \u001b[0;31m# output = self.linear1(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  output[:, 0, :].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "model = DistilBERTForBinaryClassification(1)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=2e-5)\n",
    "train_model(model, train_dl, val_dl, optim, device, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlTnImu_i4XE",
    "outputId": "14796cba-6c96-4e44-b3e3-1c347acbc095"
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate(model, test_dl, device)\n",
    "print(f'test_loss: {test_loss}')\n",
    "print(f'test_accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TU4tx6ui4XE"
   },
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "for idx, batch in enumerate(test_dl):\n",
    "    if idx % 50 == 0:\n",
    "        print(f'processing batch n. {idx}')\n",
    "    preds = model(batch['input_ids'], batch['attention_mask'])\n",
    "    preds = preds.cpu().round().flatten().int().tolist()  # Surely there's a better way\n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(batch['labels'].cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(all_preds[:20])\n",
    "print(all_labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84     12500\n",
      "           1       0.84      0.84      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n",
      "\n",
      "\n",
      "f1_score: 0.8423118516441702\n",
      "accuracy_score: 0.84252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(\"\\n\")\n",
    "print(f'f1_score: {f1_score(all_labels, all_preds)}')\n",
    "print(f'accuracy_score: {accuracy_score(all_labels, all_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
